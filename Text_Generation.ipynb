{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Generation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqjzVE/6GHRtCbzt0qZpWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yacinebouaouni/Text-Generation-RNN-Pytorch/blob/master/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTZ4Ri6GkXFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeSn8WbblAY1",
        "colab_type": "text"
      },
      "source": [
        "# 1-Loading the text data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg7Yua_gk9Xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/anna.txt','r') as file:\n",
        "  text=file.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv4axjrZlRFM",
        "colab_type": "code",
        "outputId": "03a80c9f-c704-42c8-ac50-2219da676c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "## Encode all the caracters of the text as integers:\n",
        "\n",
        "char=tuple(set(text)) #This will give all the unique caracters of the text\n",
        "int2char=dict(enumerate(char))#dictionary of int keys\n",
        "char2int={ch:ii for ii,ch in int2char.items()} #dictionary of char keys\n",
        "encoded=np.array([char2int[ch] for ch in text])\n",
        "print('the number of characters in the text : '+str(encoded.shape))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of characters in the text : (1985223,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CdFKWyHFjWN",
        "colab_type": "text"
      },
      "source": [
        "#2-Preprocessing the data:\n",
        "\n",
        "---\n",
        "The input of LSTM is a one hot encoded vector so we have to encode each caracter with a vector full of zeros and has a 1 in the corresponding index:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOMvC87_sJQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr,n_labels):\n",
        "\n",
        "  \"\"\"\n",
        "  Function takes as input encoded text in integers and return it in one hot\n",
        "  arr is the array of encoded text in integers\n",
        "  n_labels:number of labels (size of the encoded vector)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  shape=(np.multiply(*arr.shape),n_labels) #*arr.shape ==> *args of the method multiply ==> multiply dimension1*dimension2 \n",
        "  one_hot=np.zeros(shape=shape,dtype=np.float32)\n",
        "  one_hot[range(arr.shape[0]),arr.flatten()]=1\n",
        "\n",
        "  return one_hot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6wyUDL6QCxq",
        "colab_type": "text"
      },
      "source": [
        "#3-Creating Mini batches:\n",
        "---\n",
        "\n",
        "* The text will be split into Batches (K batches) and complete bacthes!!\n",
        "* Each Batch contains N sequence (batch_size=N)\n",
        "* Each sequence contains M caracter (step) \n",
        "* The last batch will be connected to the first one (last column )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOSdwWsNQ5tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_batches(arr,nb_seq,nb_step):\n",
        "\n",
        "  \"\"\"\n",
        "       Create a generator that returns batches of size\n",
        "       n_seqs x n_steps from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       n_seqs: Batch size, the number of sequences per batch\n",
        "       n_steps: Number of sequence steps per batch\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  batch_size=nb_seq*nb_step\n",
        "\n",
        "  #Generate complete batches\n",
        "  K=len(arr)//batch_size    \n",
        "  arr=arr[:K*batch_size]\n",
        "\n",
        "  #Reshape it into n_seq rows n_seq is the number of sequence/batch\n",
        "  arr=arr.reshape((nb_seq,-1)) \n",
        "\n",
        "  #Create the batch\n",
        "\n",
        "  for n in range(0,arr.shape[1],nb_step):\n",
        "\n",
        "    input=arr[:,n:n+nb_step]\n",
        "    target=np.zeros_like(input)\n",
        "    \n",
        "    #The target is the next caracter of the input so we will shift it\n",
        "    try:\n",
        "\n",
        "      target[:,:-1],target[:,-1]=input[:,1:],arr[:,n+nb_step]\n",
        "    \n",
        "    except IndexError: #This is the case of the last batch The last column of x correspond to the last column of data==> y become the first column\n",
        "\n",
        "      target[:,:-1],target[:,-1]=input[:,1:],arr[:,0]\n",
        "\n",
        "    yield input,target\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc6b1iUTebk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "4f02d9b2-56a6-4d94-ef8e-ca2561a824eb"
      },
      "source": [
        "batches=create_batches(encoded,10,50)\n",
        "x,y = next(batches)\n",
        "print('x\\n', x[:, :10])\n",
        "print('\\ny\\n', y[:,:10])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[36 24 58 33 15 56 53  1  4  0]\n",
            " [ 1 58  6  1 32 19 15  1 77 19]\n",
            " [57  3 32 79  0  0 23 28 56  2]\n",
            " [32  1 46 22 53  3 32 77  1 24]\n",
            " [ 1  3 15  1  3  2 54  1  2  3]\n",
            " [ 1 73 15  1 66 58  2  0 19 32]\n",
            " [24 56 32  1  7 19  6 56  1 78]\n",
            " [25  1 38 22 15  1 32 19 66  1]\n",
            " [15  1  3  2 32 10 15 79  1 82]\n",
            " [ 1  2 58  3 46  1 15 19  1 24]]\n",
            "\n",
            "y\n",
            " [[24 58 33 15 56 53  1  4  0  0]\n",
            " [58  6  1 32 19 15  1 77 19  3]\n",
            " [ 3 32 79  0  0 23 28 56  2 54]\n",
            " [ 1 46 22 53  3 32 77  1 24  3]\n",
            " [ 3 15  1  3  2 54  1  2  3 53]\n",
            " [73 15  1 66 58  2  0 19 32 12]\n",
            " [56 32  1  7 19  6 56  1 78 19]\n",
            " [ 1 38 22 15  1 32 19 66  1  2]\n",
            " [ 1  3  2 32 10 15 79  1 82 24]\n",
            " [ 2 58  3 46  1 15 19  1 24 56]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2emJfsDLATXq",
        "colab_type": "text"
      },
      "source": [
        "#4-Create the structure of the Model:\n",
        "\n",
        "* Create and store the necessary dictionaries (this has been done for you)\n",
        "* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n",
        "* Define a dropout layer with `dropout_prob`\n",
        "* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n",
        "* Finally, initialize the weights (again, this has been given)\n",
        "\n",
        "Note that some parameters have been named and given in the `__init__` function, and we use them and store them by doing something like `self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFHs70xH7GBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class charRNN(nn.Module):\n",
        "\n",
        "  def __init__(self,chars,n_steps,n_hidden,n_layers,drop_prob,batch_first=True):\n",
        "\n",
        "    super(charRNN,self).__init__()\n",
        "\n",
        "    self.drop_prob=drop_prob\n",
        "    self.n_hidden=n_hidden\n",
        "    self.n_layers=n_layers\n",
        "\n",
        "    #Creating characters dictionaries\n",
        "\n",
        "    self.chars=chars\n",
        "    self.int2char=dict(enumerate(self.chars))#dictionary of int keys\n",
        "    self.char2int={ch:ii for ii,ch in int2char.items()} #dictionary of char keys\n",
        "\n",
        "    #Define LSTM\n",
        "\n",
        "    self.lstm=nn.LSTM(input_size=len(int2char),hidden_size=self.n_hidden,num_layers=self.n_layers,batch_first=batch_first,dropout=drop_prob)\n",
        "\n",
        "    #Dropout layer \n",
        "\n",
        "    self.dropout=nn.Dropout(drop_prob)\n",
        "\n",
        "    #Fully connected layer predicts the probability for each char \n",
        "    \n",
        "    self.fc1=nn.Linear(in_features=n_hidden,out_features=len(self.int2char))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vktgME0wLIvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}